{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21f67623",
   "metadata": {},
   "source": [
    "Real-World Business Scenario: Performance Benchmarking\n",
    "\n",
    "Requirement: The operations team needs to identify \"Extreme Weather Rides.\" We must:\n",
    "\n",
    "1. Filter: Only analyze high-volume days (days with > 5,000 rides) to ensure the baseline is statistically significant.\n",
    "\n",
    "2. Transform: For every individual ride, calculate how much the temperature deviated from the average temperature of that specific day.\n",
    "\n",
    "3. Aggregate (Collapse): Produce a daily summary report showing the total ride count and the single highest temperature deviation recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cde7e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Production Business Report: Weather Anomalies ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>total_rides</th>\n",
       "      <th>max_temp_deviation</th>\n",
       "      <th>primary_weather</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>2016-06-30</td>\n",
       "      <td>60</td>\n",
       "      <td>9904.795000</td>\n",
       "      <td>mostlycloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2016-05-23</td>\n",
       "      <td>52</td>\n",
       "      <td>23.823077</td>\n",
       "      <td>partlycloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>2017-05-15</td>\n",
       "      <td>66</td>\n",
       "      <td>22.693939</td>\n",
       "      <td>cloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>2016-04-18</td>\n",
       "      <td>58</td>\n",
       "      <td>21.827586</td>\n",
       "      <td>partlycloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>2016-06-10</td>\n",
       "      <td>87</td>\n",
       "      <td>21.601149</td>\n",
       "      <td>partlycloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>2016-07-30</td>\n",
       "      <td>51</td>\n",
       "      <td>3.925490</td>\n",
       "      <td>mostlycloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>52</td>\n",
       "      <td>3.138462</td>\n",
       "      <td>mostlycloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>2016-09-29</td>\n",
       "      <td>53</td>\n",
       "      <td>2.828302</td>\n",
       "      <td>cloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>2016-09-23</td>\n",
       "      <td>69</td>\n",
       "      <td>2.724638</td>\n",
       "      <td>cloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>2017-10-12</td>\n",
       "      <td>81</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>cloudy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  total_rides  max_temp_deviation primary_weather\n",
       "119  2016-06-30           60         9904.795000    mostlycloudy\n",
       "95   2016-05-23           52           23.823077    partlycloudy\n",
       "202  2017-05-15           66           22.693939          cloudy\n",
       "91   2016-04-18           58           21.827586    partlycloudy\n",
       "105  2016-06-10           87           21.601149    partlycloudy\n",
       "..          ...          ...                 ...             ...\n",
       "139  2016-07-30           51            3.925490    mostlycloudy\n",
       "75   2015-10-01           52            3.138462    mostlycloudy\n",
       "179  2016-09-29           53            2.828302          cloudy\n",
       "175  2016-09-23           69            2.724638          cloudy\n",
       "310  2017-10-12           81            1.666667          cloudy\n",
       "\n",
       "[320 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset from the MDA Repository Knowledge Source\n",
    "def generate_weather_anomaly_report(path: str) -> pd.DataFrame:\n",
    "    return (\n",
    "        pd.read_csv(path)\n",
    "        # 0. INGESTION & TYPE OPTIMIZATION (The Logic Standard)\n",
    "        .assign(\n",
    "            starttime=lambda x: pd.to_datetime(x['starttime']),\n",
    "            date=lambda x: x['starttime'].dt.date\n",
    "        )\n",
    "        .loc[:, ['date', 'events', 'temperature', 'wind_speed']]\n",
    "        \n",
    "        # 1. FILTER: Data Quality Pruning (The \"Bouncer\")\n",
    "        # Rule: Drop any day that doesn't have at least 50 rides.\n",
    "        # This prevents outliers on low-volume days from skewing our logic.\n",
    "        .groupby('date')\n",
    "        .filter(lambda x: len(x) > 50)\n",
    "        \n",
    "        # 2. TRANSFORM: Feature Engineering (The \"Broadcaster\")\n",
    "        # Rule: Calculate the daily average temp and map it back to every ride.\n",
    "        # This allows row-level comparison without losing the ride details.\n",
    "        .assign(\n",
    "            daily_avg_temp=lambda x: x.groupby('date')['temperature'].transform('mean'),\n",
    "            temp_anomaly=lambda x: (x['temperature'] - x['daily_avg_temp']).abs()\n",
    "        )\n",
    "        \n",
    "        # 3. AGGREGATE: Final Reporting (The \"Collapse\")\n",
    "        # Rule: Collapse the thousands of rides into one summary row per day.\n",
    "        .groupby('date')\n",
    "        .agg(\n",
    "            total_rides=('temperature', 'size'),\n",
    "            max_temp_deviation=('temp_anomaly', 'max'),\n",
    "            primary_weather=('events', lambda x: x.value_counts().index[0])\n",
    "        )\n",
    "        .reset_index()\n",
    "        # Sort by the most \"anomalous\" days\n",
    "        .sort_values('max_temp_deviation', ascending=False)\n",
    "    )\n",
    "\n",
    "# Execution\n",
    "report = generate_weather_anomaly_report('../data/bikes.csv')\n",
    "\n",
    "print(\"--- Production Business Report: Weather Anomalies ---\")\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "baeec1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# INGESTION & TYPE OPTIMIZATION\n",
    "# ------------------------------------------------------------------------------\n",
    "# Architect's Note: \n",
    "# - We optimize strings to 'category' for group-heavy operations.\n",
    "# - 'cancelled' is cast to bool for memory efficiency.\n",
    "cols = ['date', 'airline', 'origin', 'dest', 'dep_time', 'arr_time',\n",
    "        'cancelled', 'air_time', 'distance', 'carrier_delay']\n",
    "\n",
    "flights = (pd.read_csv('../data/flights.csv', usecols=cols)\n",
    "    .assign(\n",
    "        date=lambda x: pd.to_datetime(x['date']),\n",
    "        airline=lambda x: x['airline'].astype('category'),\n",
    "        origin=lambda x: x['origin'].astype('category'),\n",
    "        dest=lambda x: x['dest'].astype('category'),\n",
    "        cancelled=lambda x: x['cancelled'].astype(bool)\n",
    "    )\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# EXERCISE 1: First and Last per Airline\n",
    "# ------------------------------------------------------------------------------\n",
    "first_last_airline = (flights\n",
    "    .groupby('airline', observed=True)\n",
    "    .nth([0, -1]).sort_values('airline',ascending=True)\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# EXERCISE 2: 500th Flight per Route\n",
    "# ------------------------------------------------------------------------------\n",
    "# Architect's Note: .nth is 0-indexed; 500th flight is index 499.\n",
    "five_hundredth_flight = (flights\n",
    "    .groupby(['origin', 'dest'], observed=True)\n",
    "    .nth(499)\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# EXERCISE 3: Date of 10th Cancelled Flight per Airline\n",
    "# ------------------------------------------------------------------------------\n",
    "tenth_cancelled_date = (flights\n",
    "    .query('cancelled == True')\n",
    "    .groupby('airline', observed=True)\n",
    "    .nth(9)\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# EXERCISE 4: Avg Delay for Routes with > 300 Flights\n",
    "# ------------------------------------------------------------------------------\n",
    "avg_delay_busy_routes = (\n",
    "    flights\n",
    "    # 1. Filter out low-volume routes (The Bouncer)\n",
    "    .groupby(['origin', 'dest'], observed=True)\n",
    "    .filter(lambda x: len(x) > 300)\n",
    "    \n",
    "    # 2. Collapse remaining high-volume routes into a summary report\n",
    "    .groupby(['origin', 'dest'], observed=True)\n",
    "    .agg(\n",
    "        avg_carrier_delay=('carrier_delay', 'mean'),\n",
    "        flight_count=('carrier_delay', 'size')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17b235d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Ex 1: Custom Binning (0-100, 101-1000, 1001+) ---\n",
      "tripduration\n",
      "Short (0-100)          242\n",
      "Medium (101-1000)    39669\n",
      "Long (1001+)         10178\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Ex 2: Equal Width Binning (Does it make sense?) ---\n",
      "tripduration\n",
      "(-26.128, 17285.6]    50060\n",
      "(17285.6, 34511.2]       11\n",
      "(34511.2, 51736.8]        9\n",
      "(51736.8, 68962.4]        3\n",
      "(68962.4, 86188.0]        6\n",
      "Name: count, dtype: int64\n",
      "Architect's Verdict: NO. The data is heavily right-skewed. Bin 1 contains 99% of data.\n",
      "\n",
      "--- Ex 3: Equal Frequency Binning (Quantiles) ---\n",
      "tripduration\n",
      "(59.999, 317.0]      10043\n",
      "(317.0, 480.0]       10011\n",
      "(480.0, 682.0]       10024\n",
      "(682.0, 1007.0]       9997\n",
      "(1007.0, 86188.0]    10014\n",
      "Name: count, dtype: int64\n",
      "Architect's Verdict: YES. This creates balanced groups for comparison.\n",
      "\n",
      "--- Ex 4: Duration vs Temperature Crosstab ---\n",
      "temperature   Coldest  Cold   Med   Hot  Hottest\n",
      "tripduration                                    \n",
      "Shortest         2712  2204  1931  1670     1526\n",
      "Short            2412  1947  2068  1927     1657\n",
      "Med              2151  2103  2067  1917     1786\n",
      "Long             1832  1940  2180  2101     1944\n",
      "Longest          1458  1754  2328  2331     2143\n",
      "\n",
      "--- Ex 5: Avg Duration by Gender & Temp Deciles ---\n",
      "temperature  (-9999.001, 37.0]  (37.0, 48.0]  (48.0, 55.9]\n",
      "gender                                                    \n",
      "Female              796.886049    670.031379    762.476715\n",
      "Male                586.972940    647.648968    622.234574\n",
      "\n",
      "--- Ex 6: Semantic Weather Buckets (with NaNs) ---\n",
      "temperature\n",
      "Cold     6434\n",
      "Cool     8483\n",
      "Mild    14819\n",
      "Warm    18355\n",
      "Hot      1998\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nealv\\AppData\\Local\\Temp\\ipykernel_55000\\685940685.py:82: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n",
      "  return df.pivot_table(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# SETUP: Load and Validate Data\n",
    "# ------------------------------------------------------------------------------\n",
    "bikes = pd.read_csv('../data/bikes.csv')\n",
    "\n",
    "# Architect's Note:\n",
    "# Binning transforms continuous numerical data into discrete categories.\n",
    "# This is crucial for handling outliers, non-linear relationships, and simplifying reporting.\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# EXERCISE 1: Custom Binning (0-100, 101-1000, 1001+)\n",
    "# ------------------------------------------------------------------------------\n",
    "def classify_trip_duration(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Categorizes trips into custom logical buckets.\n",
    "    Explanation:\n",
    "    - pd.cut() allows defining specific boundaries.\n",
    "    - We use -1 instead of 0 for the lower bound to strictly include 0 if present.\n",
    "    - Labels make the result human-readable immediately.\n",
    "    \"\"\"\n",
    "    return pd.cut(\n",
    "        df['tripduration'],\n",
    "        bins=[-1, 100, 1000, np.inf],\n",
    "        labels=['Short (0-100)', 'Medium (101-1000)', 'Long (1001+)']\n",
    "    ).value_counts().sort_index()\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# EXERCISE 2: Equal-Width Binning\n",
    "# ------------------------------------------------------------------------------\n",
    "def equal_width_analysis(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Cuts data into 5 bins of equal 'range' (width).\n",
    "    Explanation:\n",
    "    - bins=5 calculates (max - min) / 5.\n",
    "    - ISSUE: In distributions with outliers (power law), this is often useless.\n",
    "      Most data clusters in the first bin, while empty bins stretch to the outliers.\n",
    "    \"\"\"\n",
    "    return pd.cut(df['tripduration'], bins=5).value_counts().sort_index()\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# EXERCISE 3: Equal-Frequency (Quantile) Binning\n",
    "# ------------------------------------------------------------------------------\n",
    "def equal_freq_analysis(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Cuts data into 5 bins with equal 'counts' (quantiles).\n",
    "    Explanation:\n",
    "    - pd.qcut() splits data so each bin has ~20% of the rows.\n",
    "    - RESULT: Much more useful for skewed data (like trip durations) as it \n",
    "      reveals the distribution relative to the population density.\n",
    "    \"\"\"\n",
    "    return pd.qcut(df['tripduration'], q=5).value_counts().sort_index()\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# EXERCISE 4: Bivariate Quantile Analysis (Crosstab)\n",
    "# ------------------------------------------------------------------------------\n",
    "def duration_temp_crosstab(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyzes the relationship between Temperature and Trip Duration.\n",
    "    Explanation:\n",
    "    - We discretize BOTH variables into quantiles (low, med, high, etc.).\n",
    "    - pd.crosstab() creates a frequency matrix (Heatmap data).\n",
    "    - Pattern Search: Do long trips happen more in mild weather?\n",
    "    \"\"\"\n",
    "    return pd.crosstab(\n",
    "        index=pd.qcut(df['tripduration'], q=5, labels=['Shortest', 'Short', 'Med', 'Long', 'Longest']),\n",
    "        columns=pd.qcut(df['temperature'], q=5, labels=['Coldest', 'Cold', 'Med', 'Hot', 'Hottest'])\n",
    "    )\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# EXERCISE 5: Pivot Table with Binned Columns\n",
    "# ------------------------------------------------------------------------------\n",
    "def pivot_duration_gender_temp(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates Average Trip Duration by Gender across Temperature Deciles.\n",
    "    Explanation:\n",
    "    - We bin temperature into 10 buckets (deciles) inside the pivot_table call.\n",
    "    - This creates a detailed profile of how behavior changes with weather.\n",
    "    \"\"\"\n",
    "    return df.pivot_table(\n",
    "        index='gender',\n",
    "        columns=pd.qcut(df['temperature'], q=10),\n",
    "        values='tripduration',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# EXERCISE 6: Handling Data Quality & Contextual Binning\n",
    "# ------------------------------------------------------------------------------\n",
    "def clean_and_bin_temperature(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    1. Identifies and removes the anomaly (9999 or similar).\n",
    "    2. Bins valid data into semantic categories (Cold -> Hot).\n",
    "    3. Counts occurrences, explicitly tracking Missing Values (NaN).\n",
    "    \"\"\"\n",
    "    # 1. Clean: Replace impossible temps (e.g., > 150F) with NaN\n",
    "    # Architect's Note: Using .where() for vectorized replacement\n",
    "    clean_temp = df['temperature'].where(df['temperature'] < 150, np.nan)\n",
    "    \n",
    "    # 2. Bin: Define semantic boundaries (Fahrenheit assumptions)\n",
    "    # Cold: <40, Cool: 40-55, Mild: 55-70, Warm: 70-85, Hot: >85\n",
    "    return pd.cut(\n",
    "        clean_temp,\n",
    "        bins=[-np.inf, 40, 55, 70, 85, np.inf],\n",
    "        labels=['Cold', 'Cool', 'Mild', 'Warm', 'Hot']\n",
    "    ).value_counts(dropna=False).sort_index() # dropna=False keeps NaNs visible\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# EXECUTION & ANALYSIS\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"--- Ex 1: Custom Binning (0-100, 101-1000, 1001+) ---\")\n",
    "print(classify_trip_duration(bikes))\n",
    "\n",
    "print(\"\\n--- Ex 2: Equal Width Binning (Does it make sense?) ---\")\n",
    "print(equal_width_analysis(bikes))\n",
    "print(\"Architect's Verdict: NO. The data is heavily right-skewed. Bin 1 contains 99% of data.\")\n",
    "\n",
    "print(\"\\n--- Ex 3: Equal Frequency Binning (Quantiles) ---\")\n",
    "print(equal_freq_analysis(bikes))\n",
    "print(\"Architect's Verdict: YES. This creates balanced groups for comparison.\")\n",
    "\n",
    "print(\"\\n--- Ex 4: Duration vs Temperature Crosstab ---\")\n",
    "print(duration_temp_crosstab(bikes))\n",
    "\n",
    "print(\"\\n--- Ex 5: Avg Duration by Gender & Temp Deciles ---\")\n",
    "print(pivot_duration_gender_temp(bikes).iloc[:, :3]) # Showing first 3 deciles for brevity\n",
    "\n",
    "print(\"\\n--- Ex 6: Semantic Weather Buckets (with NaNs) ---\")\n",
    "print(clean_and_bin_temperature(bikes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee35d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>B42</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C148</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass  \\\n",
       "0              1         0       3   \n",
       "1              2         1       1   \n",
       "2              3         1       3   \n",
       "3              4         1       1   \n",
       "4              5         0       3   \n",
       "..           ...       ...     ...   \n",
       "886          887         0       2   \n",
       "887          888         1       1   \n",
       "888          889         0       3   \n",
       "889          890         1       1   \n",
       "890          891         0       3   \n",
       "\n",
       "                                                  Name     Sex   Age  SibSp  \\\n",
       "0                              Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                               Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                             Allen, Mr. William Henry    male  35.0      0   \n",
       "..                                                 ...     ...   ...    ...   \n",
       "886                              Montvila, Rev. Juozas    male  27.0      0   \n",
       "887                       Graham, Miss. Margaret Edith  female  19.0      0   \n",
       "888           Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN      1   \n",
       "889                              Behr, Mr. Karl Howell    male  26.0      0   \n",
       "890                                Dooley, Mr. Patrick    male  32.0      0   \n",
       "\n",
       "     Parch            Ticket     Fare Cabin Embarked  \n",
       "0        0         A/5 21171   7.2500   NaN        S  \n",
       "1        0          PC 17599  71.2833   C85        C  \n",
       "2        0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3        0            113803  53.1000  C123        S  \n",
       "4        0            373450   8.0500   NaN        S  \n",
       "..     ...               ...      ...   ...      ...  \n",
       "886      0            211536  13.0000   NaN        S  \n",
       "887      0            112053  30.0000   B42        S  \n",
       "888      2        W./C. 6607  23.4500   NaN        S  \n",
       "889      0            111369  30.0000  C148        C  \n",
       "890      0            370376   7.7500   NaN        Q  \n",
       "\n",
       "[891 rows x 12 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
