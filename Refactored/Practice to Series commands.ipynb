{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "666ac8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audit Started. Total Records: 50089\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# DATA INGESTION\n",
    "# We load the full dataset but will isolate specific Series for this Master Class.\n",
    "df = pd.read_csv('../data/bikes.csv')\n",
    "\n",
    "# ISOLATING RAW SERIES FOR THE CASE STUDY\n",
    "duration_series: pd.Series = df['tripduration']\n",
    "weather_series: pd.Series = df['temperature']\n",
    "event_series: pd.Series = df['events']\n",
    "station_series: pd.Series = df['from_station_name']\n",
    "time_series: pd.Series = df['starttime']\n",
    "\n",
    "print(f\"Audit Started. Total Records: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11a1473f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        mostlycloudy\n",
       "1        partlycloudy\n",
       "2        mostlycloudy\n",
       "3        mostlycloudy\n",
       "4        partlycloudy\n",
       "             ...     \n",
       "50084    partlycloudy\n",
       "50085    partlycloudy\n",
       "50086    partlycloudy\n",
       "50087    partlycloudy\n",
       "50088    partlycloudy\n",
       "Name: events, Length: 50089, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a6f787",
   "metadata": {},
   "source": [
    "Chapter 1: The Integrity Check (Numeric Methods)\n",
    "The Business Question: Our pricing model relies on tripduration. However, we suspect sensor errors are producing negative values or impossibly long rides (outliers). How do we stabilize this metric?\n",
    "\n",
    "In production, raw means are dangerous. We must sanitize the distribution first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78e78667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Mean: 716.87\n",
      "Robust Mean (Clipped): 679.31\n"
     ]
    }
   ],
   "source": [
    "# 1. BASIC DIAGNOSTICS\n",
    "# Standard: Use describe() to get the 5-number summary immediately.\n",
    "stats_summary = duration_series.describe(percentiles=[0.01, 0.5, 0.99])\n",
    "\n",
    "# 2. NUMERIC SANITATION PIPELINE\n",
    "# Goal: Create a \"Trusted\" duration series for modeling.\n",
    "clean_duration = (\n",
    "    duration_series\n",
    "    # Handling Negatives: Use .abs() to fix sign errors from sensors\n",
    "    .abs()\n",
    "    \n",
    "    # Handling Outliers: The \"Floor and Ceiling\" approach.\n",
    "    # We clip the top 1% to prevent a single 10-day ride from skewing the mean.\n",
    "    .clip(lower=60, upper=duration_series.quantile(0.99))\n",
    ")\n",
    "\n",
    "# 3. AGGREGATION\n",
    "# Now we calculate robust metrics on the cleaned series.\n",
    "avg_duration = clean_duration.mean()\n",
    "median_duration = clean_duration.median()\n",
    "\n",
    "print(f\"Original Mean: {duration_series.mean():.2f}\")\n",
    "print(f\"Robust Mean (Clipped): {avg_duration:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba7ed13",
   "metadata": {},
   "source": [
    "Chapter 2: The Missing Data Mystery (Missing Value Methods)\n",
    "The Business Question: The events sensor (Rain, Snow, etc.) only records data when something happens. \"Sunny\" days are recorded as NaN. We need to fill these gaps without introducing bias.\n",
    "\n",
    "Missing data is not always \"bad\"; sometimes it is \"informative.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "606aebd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Events: 0.0%\n",
      "Imputed 'Clear' Events: 0\n"
     ]
    }
   ],
   "source": [
    "# 1. IDENTIFYING THE GAP\n",
    "# Standard: Check the percentage of missing data.\n",
    "missing_pct = event_series.isna().mean() * 100\n",
    "\n",
    "# 2. CONTEXTUAL FILLING\n",
    "# Logic: If 'events' is null, it implies 'Clear' weather in this specific context.\n",
    "filled_events = (\n",
    "    event_series\n",
    "    # Validation: Check unique values before filling\n",
    "    .fillna('Clear')\n",
    ")\n",
    "\n",
    "# 3. DROPPING STRATEGIES (Scenario: Temperature)\n",
    "# Logic: If temperature is missing, we cannot guess it. We must flag it or drop it.\n",
    "valid_temps = (\n",
    "    weather_series\n",
    "    .dropna() \n",
    ")\n",
    "\n",
    "print(f\"Missing Events: {missing_pct:.1f}%\")\n",
    "print(f\"Imputed 'Clear' Events: {(filled_events == 'Clear').sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6d328a",
   "metadata": {},
   "source": [
    "Chapter 3: The Leaderboard (Sorting, Ranking & Uniqueness)\n",
    "The Business Question: Marketing wants to send \"Gold User\" coupons to riders with the longest trips. We also need to identify the busiest starting stations.\n",
    "\n",
    "We need to rank data relative to itself and count distinct occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e4102cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Size: 600 Stations\n",
      "Top Station Traffic:\n",
      " from_station_name\n",
      "Clinton St & Washington Blvd    0.017549\n",
      "Canal St & Adams St             0.014993\n",
      "Clinton St & Madison St         0.013436\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 1. UNIQUENESS\n",
    "# How many unique stations are there?\n",
    "unique_stations_count = station_series.nunique()\n",
    "\n",
    "# 2. FREQUENCY DISTRIBUTION\n",
    "# What are the top 3 busiest stations?\n",
    "# Standard: normalize=True gives relative frequency (percentages).\n",
    "top_stations = station_series.value_counts(normalize=True).head(3)\n",
    "\n",
    "# 3. RANKING\n",
    "# Logic: Rank trips by duration. 'min' method assigns the same rank to ties (e.g., tie for 1st place).\n",
    "# We want the longest trips (ascending=False).\n",
    "trip_ranks = clean_duration.rank(method='min', ascending=False)\n",
    "\n",
    "# 4. SELECTING THE \"WINNERS\" (nlargest)\n",
    "# Get the exact values of the top 5 longest rides.\n",
    "top_5_durations = clean_duration.nlargest(5)\n",
    "\n",
    "print(f\"Network Size: {unique_stations_count} Stations\")\n",
    "print(\"Top Station Traffic:\\n\", top_stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74ab7c5",
   "metadata": {},
   "source": [
    "Chapter 4: Trends & Anomalies (Advanced Series Methods)\n",
    "The Business Question: We need to detect sudden shifts in weather (temperature) to warn riders. Also, we need to perform a random audit of 10% of our transaction IDs.\n",
    "\n",
    "This requires looking at \"row vs. previous row\" logic and random sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9720d394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coldest Ride Index: 27168 (-9999.0 F)\n",
      "Audit Sample Size: 5009\n"
     ]
    }
   ],
   "source": [
    "# 1. DELTA ANALYSIS (diff & pct_change)\n",
    "# Logic: Calculate the temperature drop/rise from the previous ride record.\n",
    "temp_change = weather_series.diff()\n",
    "temp_pct_change = weather_series.pct_change()\n",
    "\n",
    "# 2. FINDING EXTREMES (idxmin / idxmax)\n",
    "# Logic: Which specific record (Index ID) had the coldest temperature?\n",
    "coldest_ride_index = weather_series.idxmin()\n",
    "coldest_temp_value = weather_series.loc[coldest_ride_index]\n",
    "\n",
    "# 3. DATA REPLACEMENT\n",
    "# Scenario: A sensor typo labeled some events 'rain' (lowercase) instead of 'Rain'.\n",
    "corrected_events = event_series.replace({'rain': 'Rain', 'snow': 'Snow'})\n",
    "\n",
    "# 4. RANDOM SAMPLING\n",
    "# Logic: Pull 10% of data for the audit, ensuring reproducibility with random_state.\n",
    "audit_sample = duration_series.sample(frac=0.10, random_state=42)\n",
    "\n",
    "print(f\"Coldest Ride Index: {coldest_ride_index} ({coldest_temp_value} F)\")\n",
    "print(f\"Audit Sample Size: {len(audit_sample)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6ffcb3",
   "metadata": {},
   "source": [
    "Chapter 5: Text Wrangling (String Methods)\n",
    "The Business Question: The station names are messy. We need to standardize them for the app display. Specifically, we want to find all stations located near a \"Park\".\n",
    "\n",
    "Vectorized string operations (.str) are the only acceptable way to process text in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e81d0219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Park Stations Found: 1259\n",
      "Sample Neighborhoods: ['Madison St' 'State St & 16th St' 'State St & 16th St']\n"
     ]
    }
   ],
   "source": [
    "# 1. STANDARDIZATION\n",
    "# Logic: Lowercase everything and strip whitespace to prevent join errors.\n",
    "clean_names = (\n",
    "    station_series\n",
    "    .astype(str) # Safety cast\n",
    "    .str.lower()\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "# 2. PATTERN MATCHING (contains)\n",
    "# Logic: Find boolean mask for stations containing 'park'.\n",
    "is_park_station = clean_names.str.contains('park', regex=False)\n",
    "\n",
    "# 3. EXTRACTION (split)\n",
    "# Logic: The data format is \"StationName (Neighborhood)\". We want just the Neighborhood.\n",
    "# We split by '(' and take the second part (index 1).\n",
    "neighborhoods = (\n",
    "    station_series\n",
    "    .str.split('(')\n",
    "    .str.get(1) # Access the second element of the list\n",
    "    .str.replace(')', '', regex=False) # Remove the closing parenthesis\n",
    ")\n",
    "\n",
    "print(f\"Park Stations Found: {is_park_station.sum()}\")\n",
    "print(\"Sample Neighborhoods:\", neighborhoods.dropna().head(3).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acb034c",
   "metadata": {},
   "source": [
    "Chapter 6: The Timeline (Datetime Methods)\n",
    "The Business Question: Usage patterns vary by day. We need to convert the raw time strings into actual dates to determine which day of the week has the highest load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db0eb56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Busiest Day: Tuesday\n",
      "Peak Hour: 17\n"
     ]
    }
   ],
   "source": [
    "# 1. CONVERSION\n",
    "# Logic: Coerce errors to NaT (Not a Time) to avoid crashing on garbage data.\n",
    "# Note: In production, explicitly specify 'format' if known for 10x speedup.\n",
    "dt_series = pd.to_datetime(time_series, errors='coerce')\n",
    "\n",
    "# 2. COMPONENT EXTRACTION\n",
    "# Logic: Extract the day name (Mon/Tue) and the hour (0-23).\n",
    "day_names = dt_series.dt.day_name()\n",
    "ride_hours = dt_series.dt.hour\n",
    "\n",
    "# 3. AGGREGATING BY TIME\n",
    "# Logic: Count rides per day name.\n",
    "weekly_traffic = day_names.value_counts()\n",
    "\n",
    "print(\"Busiest Day:\", weekly_traffic.idxmax())\n",
    "print(\"Peak Hour:\", ride_hours.mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf59ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    17\n",
       "Name: starttime, dtype: int32"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
